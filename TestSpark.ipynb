{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pyspark import SparkContext,SparkConf\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def CreateSparkContex():\n",
    "    sparkconf=SparkConf().setAppName(\"shandongprocess\").set(\"spark.ui.showConsoleProgress\",\"false\")\n",
    "    sc=SparkContext(conf=sparkconf)\n",
    "    print(\"master:\"+sc.master)\n",
    "    sc.setLogLevel(\"WARN\")\n",
    "    Setpath(sc)\n",
    "    spark = SparkSession.builder.config(conf=sparkconf).getOrCreate()\n",
    "    return sc,spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Setpath(sc):\n",
    "    global Path\n",
    "    if sc.master[:5]==\"local\":\n",
    "        Path=\"file:/F:/software/work/Python/data\"\n",
    "    else:\n",
    "        Path=\"hdfs://data\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "master:local[*]\n"
     ]
    }
   ],
   "source": [
    "sc,spark=CreateSparkContex()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here we go! file:/F:/software/work/Python/data\\metadata_trans.csv\n"
     ]
    }
   ],
   "source": [
    "readdatapath=os.path.join(Path,'metadata_trans.csv')\n",
    "print(\"Here we go!\", readdatapath)\n",
    "data=spark.read.csv(readdatapath,header=True)\n",
    "# sc.stop()\n",
    "# spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+---------+--------+--------+----+--------------+--------+----------------+-------------------+---------+-------------------+-----------------+-------------------+-----------------+----+----+------------------+--------+--------+----------+------+\n",
      "|     工单id|   产品id|是否生效|支付方式|金额|      生效时间|退订时间|        失效时间|          生效_time|退订_time|          失效_time|        留存_time|          退订_fill|    有效订购_time|类别|频道|          产品名称|  包类型| 包类型1|包类型编码|总收入|\n",
      "+-----------+---------+--------+--------+----+--------------+--------+----------------+-------------------+---------+-------------------+-----------------+-------------------+-----------------+----+----+------------------+--------+--------+----------+------+\n",
      "|18769452858|TV4KBY20Y|       1|     903|20.0|20170402092917|    null|20191205000000.0|2017-04-02 09:29:17|     null|2019-12-05 00:00:00|976.6046643518519|2019-12-05 00:00:00|976.6046643518519|电影|电影|魔百和电影连续包月|连续包月|单产品包|         2| 660.0|\n",
      "+-----------+---------+--------+--------+----+--------------+--------+----------------+-------------------+---------+-------------------+-----------------+-------------------+-----------------+----+----+------------------+--------+--------+----------+------+\n",
      "only showing top 1 row\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data.show(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.select('userid','contractid', 'payway', 'price', 'subtime_trans', 'unsubtime_trans', 'endtime_trans', 'keeptime_days', 'keepsubtime_days')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.createOrReplaceTempView(\"data_user\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "product.createOrReplaceTempView(\"product\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "product.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "data.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# contractid, status, payway, price, subtime_trans \n",
    "results = spark.sql(\"SELECT userid, contractid, payway, price/100 as yuan, keeptime_days, keepsubtime_days, cast(date_format(subtime_trans, 'yyyyMM') as int) as subtime_y, cast(date_format(subtime_trans, 'yyyyMMdd') as int) as subtime_date,  cast(date_format(unsubtime_trans, 'yyyyMMdd') as int) as unsubtime_date, cast(date_format(endtime_trans, 'yyyyMMdd') as int) as endtime_date FROM data_user\")\n",
    "results.createOrReplaceTempView(\"to_date_user\")\n",
    "results.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for por_name in ['玩吧游戏','咪咕游戏','电竞风云','成长乐园','百灵K歌']:\n",
    "        print(spark.sql(\"SELECT count(userid) FROM to_date_user where subtime_y = 201905 and contractid in (select pro_id from product where pack_type = '连续包月' and pro_name  like '%s') \" % (por_name + '%')).toPandas().values[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "spark.sql(\"SELECT count(userid) FROM to_date_user where contractid like 'TVWBY%' and subtime_y = 201906 \").count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "user_keeptime = [20190601, 20190701, 20190801, 20190901, 20191001, 20191101, 20191201] # 20190201, 20190301, 20190401, 20190501, 20190601, 20190701, 20190801, 20190901, \n",
    "for val in user_keeptime:\n",
    "    print(val)\n",
    "    for por_name in ['玩吧游戏','咪咕游戏','电竞风云','成长乐园','百灵K歌']:\n",
    "        print(spark.sql(\"SELECT count(userid) FROM to_date_user where contractid in (select pro_id from product where pack_type = '连续包月' and pro_name  like '%s') and (subtime_y = 201905 and endtime_date > %d)\" % (por_name + '%', val)).toPandas().values[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "user_keeptime_num = [0, 1, 2, 3, 4, 5, 6] # 生成各订购时间的日期条件\n",
    "for i,val in enumerate(user_keeptime_num): # 枚举遍历，i为列表索引，val列表值\n",
    "    print(val,'------------')\n",
    "    for por_name in ['新东方TV学堂','魔百和国粹','玩吧游戏','NBA续包月','咪咕游戏','魔百和纪录片','海看鼎级剧场包月','魔百和动漫','魔百和电影','魔百和热剧','电竞风云','成长乐园','百灵K歌']:\n",
    "        if i == len(user_keeptime_num)-1: # 处理边界值\n",
    "            print(spark.sql(\"SELECT count(userid) FROM to_date_user where subtime_y ==2019 and keeptime_days > %s and  contractid in (select pro_id from product where pack_type = '连续包月' and pro_name  like '%s')\" % (val*30, por_name + '%')).toPandas().values[0][0])\n",
    "        else:\n",
    "            print(spark.sql(\"SELECT count(userid) FROM to_date_user where subtime_y ==2019 and (keeptime_days > %s and keeptime_days <= %s) and  contractid in (select pro_id from product where pack_type = '连续包月' and pro_name  like '%s')\" % (val*30, user_keeptime_num[i+1]*30, por_name + '%')).toPandas().values[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "user_order_date = [0, (1, 10), (11, 20), (21, 31)] # 生成各订购时间的日期条件\n",
    "for val in user_order_date: # 枚举遍历，i为列表索引，val列表值\n",
    "    print(val,'------------')\n",
    "    for por_name in ['新东方TV学堂','魔百和国粹','玩吧游戏','NBA续包月','咪咕游戏','魔百和纪录片','海看鼎级剧场包月','魔百和动漫','魔百和电影','魔百和热剧','电竞风云','成长乐园','百灵K歌']:\n",
    "        if type(val) is int: # 处理边界值\n",
    "            print(spark.sql(\"SELECT count(userid) FROM to_date_user where subtime_y ==2019 and keeptime_days = %s and  contractid in (select pro_id from product where pack_type = '连续包月' and pro_name  like '%s')\" % (val, por_name + '%')).toPandas().values[0][0])\n",
    "        elif type(val) is tuple:\n",
    "            print(spark.sql(\"SELECT count(userid) FROM to_date_user where subtime_y ==2019 and (keeptime_days >= %s and keeptime_days <= %s) and  contractid in (select pro_id from product where pack_type = '连续包月' and pro_name  like '%s')\" % (val[0],val[1], por_name + '%')).toPandas().values[0][0])\n",
    "        else:\n",
    "            raise Exception('Value Error')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_order_times_num = [1, 2, 3, (4, 6), (7, 9), (10, float(\"inf\"))] # 生成各个时间点的条件\n",
    "for val in user_order_times_num:\n",
    "    if type(val) is int: # 单条件判断\n",
    "        user_id_data = order_times[order_times['订购次数'] == val]['userid'] # 根据订购次数，获取符合条件的用户ID\n",
    "        column_name = '%s次' % val\n",
    "    elif type(val) is tuple: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "user_order_date = [0, 10, 20, 31] # 生成各订购时间的日期条件\n",
    "for i,val in enumerate(user_order_date): # 枚举遍历，i为列表索引，val列表值\n",
    "    print(val,'------------')\n",
    "    for por_name in ['新东方TV学堂','魔百和国粹','玩吧游戏','NBA续包月','咪咕游戏','魔百和纪录片','海看鼎级剧场包月','魔百和动漫','魔百和电影','魔百和热剧','电竞风云','成长乐园','百灵K歌']:\n",
    "#         print(por_name)\n",
    "        if i == len(user_order_date)-1: # 处理边界值\n",
    "            break\n",
    "        else:\n",
    "            print(spark.sql(\"SELECT count(userid) FROM to_date_user where subtime_y ==2019 and (unsubtime_date > %s and unsubtime_date <= %s) and  contractid in (select pro_id from product where pack_type = '连续包月' and pro_name  like '%s')\" % (val,user_order_date[i+1], por_name + '%')).toPandas().values[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "user_order_date = [0, 10, 20, 31] # 生成各订购时间的日期条件\n",
    "for i,val in enumerate(user_order_date): # 枚举遍历，i为列表索引，val列表值\n",
    "    print(val,'------------')\n",
    "    for por_name in ['新东方TV学堂','魔百和国粹','玩吧游戏','NBA续包月','咪咕游戏','魔百和纪录片','海看鼎级剧场包月','魔百和动漫','魔百和电影','魔百和热剧','电竞风云','成长乐园','百灵K歌']:\n",
    "#         print(por_name)\n",
    "        if i == len(user_order_date)-1: # 处理边界值\n",
    "            break\n",
    "        else:\n",
    "            print(spark.sql(\"SELECT count(userid) FROM to_date_user where subtime_y ==2019 and (subtime_date > %s and subtime_date <= %s) and  contractid in (select pro_id from product where pack_type = '连续包月' and pro_name  like '%s')\" % (val,user_order_date[i+1], por_name + '%')).toPandas().values[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# contractid, status, payway, price, subtime_trans\n",
    "results = spark.sql(\"SELECT userid, contractid, payway, price/100 as yuan, date_format(subtime_trans, 'yyyyMM') as subtime_date, date_format(endtime_trans, 'yyyyMM') as endtime_date FROM data_user\")\n",
    "results.createOrReplaceTempView(\"to_date_user\")\n",
    "results.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = spark.sql(\"SELECT * FROM to_date_user WHERE contractid NOT IN {0}\".format(('TVDJFYLXBY29Y','TVMBHRJBY20Y','TV4KBY20Y','TV4KMBKDMBY10Y','TVMBHDMBY20Y',\n",
    "                                      'TVMBHJLPBY15Y','TVMBHXQBBY10Y','TVXDFTVXTXBY20Y','TVMBHRJBY20Y','TVMBHDMBY20Y')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b = results.where(col(\"contractid\").isin({'TVDJFYLXBY29Y','TVMBHRJBY20Y','TV4KBY20Y','TV4KMBKDMBY10Y','TVMBHDMBY20Y',\n",
    "                                      'TVMBHJLPBY15Y','TVMBHXQBBY10Y','TVXDFTVXTXBY20Y','TVMBHRJBY20Y','TVMBHDMBY20Y'}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# results.groupBy(\"subtime_date\").sum().sort(\"subtime_date\").toPandas().to_excel('./收入.xlsx',index=False) # write.csv('./收入.csv')\n",
    "b.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "user_num_dict = defaultdict(list) # 生成一个元素为列表的字典\n",
    "product_a = ['TVMBHDYDBY25Y','TV4KBY20Y','TV4KBY20Y','TV4KBY20Y','TVMBHDY120LYB','TV4KBY20Y','TV4KBY20Y','TVMBHDYRJ199LYB']\n",
    "product_b = ['TVMBHRJDBY25Y','TVMBHRJBY20Y','TVMBHRJBY20Y','TVMBHRJBY20Y','TVMBHRJ120LYB','TVMBHRJBY20Y','TVMBHRJBY20Y']\n",
    "product_c = ['TVMBHDMDBY20Y','TV4KMBKDMBY10Y','TVMBHDMBY20Y','TV4KMBKDMBY10Y','TVMBHDMBY20Y','TV4KMBKDMBY10Y','TVMBHDMBY20Y',\n",
    "             'TVMBHDM99LYB','TVMBHDM120LYB','TV4KMBKDMBY10Y','TVMBHDMBY20Y','TV4KMBKDMBY10Y','TVMBHDMBY20Y']\n",
    "for year in ['2017', '2018', '2019']:\n",
    "    print(year)\n",
    "    for month in ['01','02','03','04','05','06','07','08','09','10','11','12']:\n",
    "        datetime = year + '-' + month\n",
    "        user_num_dict['月份'].append(datetime)\n",
    "        # spark.sql(\"SELECT count(distinct('userid')) FROM to_date_user\")\n",
    "        # user_num_dict['有效去重用户数'].append(results.filter(results.subtime_date <= datetime).filter(results.endtime_date > datetime).distinct().count())\n",
    "        total = 0\n",
    "        \n",
    "        revenue_a = a.filter(a.subtime_date == datetime).where(col(\"contractid\").isin(product_c)).agg({\"yuan\": \"sum\"}).toPandas().values[0][0]\n",
    "        revenue_b = b.filter(b.subtime_date <= datetime).filter(b.endtime_date >= datetime).where(col(\"contractid\").isin(product_c)).agg(\n",
    "            {\"yuan\": \"sum\"}).toPandas().values[0][0]\n",
    "        if revenue_a is not None:\n",
    "            total+=revenue_a\n",
    "        if revenue_b is not None:\n",
    "            total+=revenue_b\n",
    "        print(total)\n",
    "        user_num_dict['收入'].append(total)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "user_num_dict = defaultdict(list) # 生成一个元素为列表的字典\n",
    "for payway in ['903', '701', '56']:\n",
    "    for year in ['2015', '2016', '2017', '2018', '2019']:\n",
    "        user_num_dict['年份'].append(year)\n",
    "        year_total = 0\n",
    "        for month in ['01','02','03','04','05','06','07','08','09','10','11','12']:\n",
    "            datetime = year + '-' + month\n",
    "            month_total = 0\n",
    "            revenue_a = a.filter(a.subtime_date == datetime).filter(a.payway == payway).agg({\"yuan\": \"sum\"}).toPandas().values[0][0]\n",
    "            revenue_b = b.filter(b.subtime_date <= datetime).filter(b.endtime_date >= datetime).filter(a.payway == payway).agg({\"yuan\": \"sum\"}).toPandas().values[0][0]\n",
    "            if revenue_a is not None:\n",
    "                month_total += revenue_a\n",
    "            if revenue_b is not None:\n",
    "                month_total += revenue_b\n",
    "            year_total += month_total\n",
    "        print(year_total)\n",
    "        user_num_dict[payway].append(year_total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "user_num_dict = defaultdict(list) # 生成一个元素为列表的字典\n",
    "for year in ['2015', '2016', '2017', '2018', '2019']:\n",
    "    user_num_dict['年份'].append(year)\n",
    "    year_total = 0\n",
    "    for month in ['01','02','03','04','05','06','07','08','09','10','11','12']:\n",
    "        datetime = year + '-' + month\n",
    "        month_total = 0\n",
    "        revenue_b = b.filter(b.subtime_date <= datetime).filter(b.endtime_date >= datetime).agg({\"yuan\": \"sum\"}).toPandas().values[0][0]\n",
    "        if revenue_b is not None:\n",
    "            month_total += revenue_b\n",
    "        year_total += month_total\n",
    "    print(year_total)\n",
    "    user_num_dict[payway].append(year_total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(user_num_dict)\n",
    "# user_num_data=pd.DataFrame(user_num_dict) # 将字典转换为DataFrame，字典元素必须为列表类型\n",
    "# user_num_data.to_excel('./各年不同订购方式的收入.xlsx',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "user_num_dict = defaultdict(list) # 生成一个元素为列表的字典\n",
    "for year in ['2015', '2016', '2017', '2018', '2019']:\n",
    "    print(year)\n",
    "    for month in ['01','02','03','04','05','06','07','08','09','10','11','12']:\n",
    "        datetime = year + '-' + month\n",
    "        user_num_dict['月份'].append(datetime)\n",
    "        # spark.sql(\"SELECT count(distinct('userid')) FROM to_date_user\")\n",
    "        # user_num_dict['有效去重用户数'].append(results.filter(results.subtime_date <= datetime).filter(results.endtime_date > datetime).distinct().count())\n",
    "        total = 0\n",
    "        revenue_a = a.filter(a.subtime_date == datetime).agg({\"yuan\": \"sum\"}).toPandas().values[0][0]\n",
    "        revenue_b = b.filter(b.subtime_date <= datetime).filter(b.endtime_date >= datetime).agg({\"yuan\": \"sum\"}).toPandas().values[0][0]\n",
    "        if revenue_a is not None:\n",
    "            total+=revenue_a\n",
    "        if revenue_b is not None:\n",
    "            total+=revenue_b\n",
    "        print(revenue_a,revenue_b,total)\n",
    "        user_num_dict['收入'].append(total)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_num_data=pd.DataFrame(user_num_dict) # 将字典转换为DataFrame，字典元素必须为列表类型\n",
    "user_num_data.to_excel('./收入.xlsx',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.udf.register(\"strLen\", (str: String) => str.length())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results.select(\"subtime\")."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results.sort(results.userid.desc(), results.contractid.asc()).show(1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
