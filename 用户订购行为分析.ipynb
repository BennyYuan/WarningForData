{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pyspark import SparkContext,SparkConf\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def CreateSparkContex():\n",
    "    sparkconf=SparkConf().setAppName(\"shandongprocess\").set(\"spark.ui.showConsoleProgress\",\"false\")\n",
    "    sc=SparkContext(conf=sparkconf)\n",
    "    print(\"master:\"+sc.master)\n",
    "    sc.setLogLevel(\"WARN\")\n",
    "    Setpath(sc)\n",
    "    spark = SparkSession.builder.config(conf=sparkconf).getOrCreate()\n",
    "    return sc,spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Setpath(sc):\n",
    "    global Path\n",
    "    if sc.master[:5]==\"local\":\n",
    "        Path=\"file:/F:/software/work/Python/data\"\n",
    "    else:\n",
    "        Path=\"hdfs://data\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "master:local[*]\n"
     ]
    }
   ],
   "source": [
    "sc,spark=CreateSparkContex()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here we go! file:/F:/software/work/Python/data\\metadata_trans.csv\n"
     ]
    }
   ],
   "source": [
    "readdatapath=os.path.join(Path,'metadata_trans.csv')\n",
    "print(\"Here we go!\", readdatapath)\n",
    "data=spark.read.csv(readdatapath,header=True)\n",
    "# sc.stop()\n",
    "# spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.createOrReplaceTempView(\"data\")\n",
    "data = spark.sql(\"SELECT * FROM data where subtime_trans < unsubtime_fill\") # 数据清洗\n",
    "data.createOrReplaceTempView(\"data_user\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(userid='18769452858', contractid='TV4KBY20Y', status='1', payway='903', price='20.0', subtime='20170402092917', unsubtime=None, endtime='20191205000000.0', subtime_trans='2017-04-02 09:29:17', unsubtime_trans=None, endtime_trans='2019-12-05 00:00:00', keeptime_days='976.6046643518519', unsubtime_fill='2019-12-05 00:00:00', effective_days='976.6046643518519', pro_class='电影', channel='电影', pro_name='魔百和电影连续包月', pack_type='连续包月', pack_type1='单产品包', pack_id='2', total_reve='660.0')]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.take(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 产品 vs 购买时间"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 每年各产品的周订购情况\n",
    "product_dict = {'产品类别': 'pro_class', '产品频道': 'channel', '产品包类型': 'pack_type'}\n",
    "for key, product in product_dict.items():\n",
    "    spark.sql(\"SELECT date_format(subtime_trans, 'yyyy') as subtime_year, %s, dayofweek(subtime_trans) as subtime_weekday, count(userid) FROM data_user group by subtime_year, %s, subtime_weekday order by subtime_year, %s, subtime_weekday asc\" % (product, product, product)).toPandas().to_excel('./每年%s的周订购情况.xlsx' % (key), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 每年各产品的月订购情况\n",
    "product_dict = {'产品类别':'pro_class', '产品频道':'channel', '产品包类型':'pack_type'}\n",
    "for key,product in product_dict.items():\n",
    "    spark.sql(\"SELECT date_format(subtime_trans, 'yyyy') as subtime_year, %s, month(subtime_trans) as subtime_month, count(userid) FROM data_user group by subtime_year, %s, subtime_month order by subtime_year, %s, subtime_month asc\" % (product, product, product)).toPandas().to_excel('./每年%s的月订购情况.xlsx' % (key),index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 每年各产品的日订购情况\n",
    "product_dict = {'产品类别':'pro_class', '产品频道':'channel', '产品包类型':'pack_type'}\n",
    "for key,product in product_dict.items():\n",
    "    spark.sql(\"SELECT date_format(subtime_trans, 'yyyy') as subtime_year, %s, day(subtime_trans) as subtime_day, count(userid) FROM data_user group by subtime_year, %s, subtime_day order by subtime_year, %s, subtime_day asc\" % (product, product, product)).toPandas().to_excel('./每年%s的日订购情况.xlsx' % (key),index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 产品 vs 退订时间"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 每年各产品的周订购情况\n",
    "product_dict = {'产品类别':'pro_class', '产品频道':'channel', '产品包类型':'pack_type'}\n",
    "for key,product in product_dict.items():\n",
    "    spark.sql(\"SELECT date_format(unsubtime_trans, 'yyyy') as unsubtime_year, %s, dayofweek(unsubtime_trans) as unsubtime_weekday, count(userid) FROM data_user where unsubtime_trans is not NULL and unsubtime_trans <= endtime_trans group by unsubtime_year, %s, unsubtime_weekday order by unsubtime_year, %s, unsubtime_weekday asc\" % (product, product, product)).toPandas().to_excel('./每年%s的周退订情况.xlsx' % (key),index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 每年各产品的月订购情况\n",
    "product_dict = {'产品类别':'pro_class', '产品频道':'channel', '产品包类型':'pack_type'}\n",
    "for key,product in product_dict.items():\n",
    "    spark.sql(\"SELECT date_format(unsubtime_trans, 'yyyy') as unsubtime_year, %s, month(unsubtime_trans) as unsubtime_month, count(userid) FROM data_user where unsubtime_trans is not NULL and unsubtime_trans <= endtime_trans group by unsubtime_year, %s, unsubtime_month order by unsubtime_year, %s, unsubtime_month asc\" % (product, product, product)).toPandas().to_excel('./每年%s的月退订情况.xlsx' % (key),index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 每年各产品的日订购情况\n",
    "product_dict = {'产品类别':'pro_class', '产品频道':'channel', '产品包类型':'pack_type'}\n",
    "for key,product in product_dict.items():\n",
    "    spark.sql(\"SELECT date_format(unsubtime_trans, 'yyyy') as unsubtime_year, %s, day(unsubtime_trans) as unsubtime_day, count(userid) FROM data_user where unsubtime_trans is not NULL and unsubtime_trans <= endtime_trans group by unsubtime_year, %s, unsubtime_day order by unsubtime_year, %s, unsubtime_day asc\" % (product, product, product)).toPandas().to_excel('./每年%s的日退订情况.xlsx' % (key),index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 退订意愿 vs 支付方式"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+------+----------------------+\n",
      "|unsubtime_year|payway|count(DISTINCT userid)|\n",
      "+--------------+------+----------------------+\n",
      "|          2018|   903|                529743|\n",
      "|          2019|   903|                967517|\n",
      "|          2017|   903|                 37099|\n",
      "|          2016|   903|                   136|\n",
      "|          2018|   701|                   540|\n",
      "|          2018|    56|                  9196|\n",
      "|          2019|   701|                  4514|\n",
      "|          2019|    56|                 11170|\n",
      "+--------------+------+----------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 不同支付方式与订购用户数的关系\n",
    "spark.sql(\"SELECT date_format(unsubtime_trans, 'yyyy') as unsubtime_year, payway, count(DISTINCT userid) FROM data_user where unsubtime_trans is not NULL and unsubtime_trans <= endtime_trans group by unsubtime_year, payway\").show(30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-------------+\n",
      "|payway|count(userid)|\n",
      "+------+-------------+\n",
      "|   903|      2371974|\n",
      "|   701|         6016|\n",
      "|    56|        26767|\n",
      "+------+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 不同支付方式与订购人次的关系\n",
    "spark.sql(\"SELECT payway, count(userid) FROM data_user where unsubtime_trans is not NULL and unsubtime_trans <= endtime_trans group by payway\").show(30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spark SQL测试"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# 每年不同status的订购情况\n",
    "spark.sql(\"SELECT count(*) FROM data_user where unsubtime_trans is not NULL and unsubtime_trans <= endtime_trans\").show(30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# contractid, status, payway, price, subtime_trans \n",
    "results = spark.sql(\"SELECT userid, contractid, payway, price/100 as yuan, keeptime_days, effective_days, cast(date_format(subtime_trans, 'yyyyMM') as int) as subtime_y, cast(date_format(subtime_trans, 'yyyyMMdd') as int) as subtime_date,  cast(date_format(unsubtime_trans, 'yyyyMMdd') as int) as unsubtime_date, cast(date_format(endtime_trans, 'yyyyMMdd') as int) as endtime_date FROM data_user\")\n",
    "results.createOrReplaceTempView(\"to_date_user\")\n",
    "results.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for por_name in ['玩吧游戏','咪咕游戏','电竞风云','成长乐园','百灵K歌']:\n",
    "        print(spark.sql(\"SELECT count(userid) FROM to_date_user where subtime_y = 201905 and contractid in (select pro_id from product where pack_type = '连续包月' and pro_name  like '%s') \" % (por_name + '%')).toPandas().values[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "spark.sql(\"SELECT count(userid) FROM to_date_user where contractid like 'TVWBY%' and subtime_y = 201906 \").count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "user_keeptime = [20190601, 20190701, 20190801, 20190901, 20191001, 20191101, 20191201] # 20190201, 20190301, 20190401, 20190501, 20190601, 20190701, 20190801, 20190901, \n",
    "for val in user_keeptime:\n",
    "    print(val)\n",
    "    for por_name in ['玩吧游戏','咪咕游戏','电竞风云','成长乐园','百灵K歌']:\n",
    "        print(spark.sql(\"SELECT count(userid) FROM to_date_user where contractid in (select pro_id from product where pack_type = '连续包月' and pro_name  like '%s') and (subtime_y = 201905 and endtime_date > %d)\" % (por_name + '%', val)).toPandas().values[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "user_keeptime_num = [0, 1, 2, 3, 4, 5, 6] # 生成各订购时间的日期条件\n",
    "for i,val in enumerate(user_keeptime_num): # 枚举遍历，i为列表索引，val列表值\n",
    "    print(val,'------------')\n",
    "    for por_name in ['新东方TV学堂','魔百和国粹','玩吧游戏','NBA续包月','咪咕游戏','魔百和纪录片','海看鼎级剧场包月','魔百和动漫','魔百和电影','魔百和热剧','电竞风云','成长乐园','百灵K歌']:\n",
    "        if i == len(user_keeptime_num)-1: # 处理边界值\n",
    "            print(spark.sql(\"SELECT count(userid) FROM to_date_user where subtime_y ==2019 and keeptime_days > %s and  contractid in (select pro_id from product where pack_type = '连续包月' and pro_name  like '%s')\" % (val*30, por_name + '%')).toPandas().values[0][0])\n",
    "        else:\n",
    "            print(spark.sql(\"SELECT count(userid) FROM to_date_user where subtime_y ==2019 and (keeptime_days > %s and keeptime_days <= %s) and  contractid in (select pro_id from product where pack_type = '连续包月' and pro_name  like '%s')\" % (val*30, user_keeptime_num[i+1]*30, por_name + '%')).toPandas().values[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "user_order_date = [0, (1, 10), (11, 20), (21, 31)] # 生成各订购时间的日期条件\n",
    "for val in user_order_date: # 枚举遍历，i为列表索引，val列表值\n",
    "    print(val,'------------')\n",
    "    for por_name in ['新东方TV学堂','魔百和国粹','玩吧游戏','NBA续包月','咪咕游戏','魔百和纪录片','海看鼎级剧场包月','魔百和动漫','魔百和电影','魔百和热剧','电竞风云','成长乐园','百灵K歌']:\n",
    "        if type(val) is int: # 处理边界值\n",
    "            print(spark.sql(\"SELECT count(userid) FROM to_date_user where subtime_y ==2019 and keeptime_days = %s and  contractid in (select pro_id from product where pack_type = '连续包月' and pro_name  like '%s')\" % (val, por_name + '%')).toPandas().values[0][0])\n",
    "        elif type(val) is tuple:\n",
    "            print(spark.sql(\"SELECT count(userid) FROM to_date_user where subtime_y ==2019 and (keeptime_days >= %s and keeptime_days <= %s) and  contractid in (select pro_id from product where pack_type = '连续包月' and pro_name  like '%s')\" % (val[0],val[1], por_name + '%')).toPandas().values[0][0])\n",
    "        else:\n",
    "            raise Exception('Value Error')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_order_times_num = [1, 2, 3, (4, 6), (7, 9), (10, float(\"inf\"))] # 生成各个时间点的条件\n",
    "for val in user_order_times_num:\n",
    "    if type(val) is int: # 单条件判断\n",
    "        user_id_data = order_times[order_times['订购次数'] == val]['userid'] # 根据订购次数，获取符合条件的用户ID\n",
    "        column_name = '%s次' % val\n",
    "    elif type(val) is tuple: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "user_order_date = [0, 10, 20, 31] # 生成各订购时间的日期条件\n",
    "for i,val in enumerate(user_order_date): # 枚举遍历，i为列表索引，val列表值\n",
    "    print(val,'------------')\n",
    "    for por_name in ['新东方TV学堂','魔百和国粹','玩吧游戏','NBA续包月','咪咕游戏','魔百和纪录片','海看鼎级剧场包月','魔百和动漫','魔百和电影','魔百和热剧','电竞风云','成长乐园','百灵K歌']:\n",
    "#         print(por_name)\n",
    "        if i == len(user_order_date)-1: # 处理边界值\n",
    "            break\n",
    "        else:\n",
    "            print(spark.sql(\"SELECT count(userid) FROM to_date_user where subtime_y ==2019 and (unsubtime_date > %s and unsubtime_date <= %s) and  contractid in (select pro_id from product where pack_type = '连续包月' and pro_name  like '%s')\" % (val,user_order_date[i+1], por_name + '%')).toPandas().values[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "user_order_date = [0, 10, 20, 31] # 生成各订购时间的日期条件\n",
    "for i,val in enumerate(user_order_date): # 枚举遍历，i为列表索引，val列表值\n",
    "    print(val,'------------')\n",
    "    for por_name in ['新东方TV学堂','魔百和国粹','玩吧游戏','NBA续包月','咪咕游戏','魔百和纪录片','海看鼎级剧场包月','魔百和动漫','魔百和电影','魔百和热剧','电竞风云','成长乐园','百灵K歌']:\n",
    "#         print(por_name)\n",
    "        if i == len(user_order_date)-1: # 处理边界值\n",
    "            break\n",
    "        else:\n",
    "            print(spark.sql(\"SELECT count(userid) FROM to_date_user where subtime_y ==2019 and (subtime_date > %s and subtime_date <= %s) and  contractid in (select pro_id from product where pack_type = '连续包月' and pro_name  like '%s')\" % (val,user_order_date[i+1], por_name + '%')).toPandas().values[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# contractid, status, payway, price, subtime_trans\n",
    "results = spark.sql(\"SELECT userid, contractid, payway, price/100 as yuan, date_format(subtime_trans, 'yyyyMM') as subtime_date, date_format(endtime_trans, 'yyyyMM') as endtime_date FROM data_user\")\n",
    "results.createOrReplaceTempView(\"to_date_user\")\n",
    "results.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = spark.sql(\"SELECT * FROM to_date_user WHERE contractid NOT IN {0}\".format(('TVDJFYLXBY29Y','TVMBHRJBY20Y','TV4KBY20Y','TV4KMBKDMBY10Y','TVMBHDMBY20Y',\n",
    "                                      'TVMBHJLPBY15Y','TVMBHXQBBY10Y','TVXDFTVXTXBY20Y','TVMBHRJBY20Y','TVMBHDMBY20Y')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b = results.where(col(\"contractid\").isin({'TVDJFYLXBY29Y','TVMBHRJBY20Y','TV4KBY20Y','TV4KMBKDMBY10Y','TVMBHDMBY20Y',\n",
    "                                      'TVMBHJLPBY15Y','TVMBHXQBBY10Y','TVXDFTVXTXBY20Y','TVMBHRJBY20Y','TVMBHDMBY20Y'}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# results.groupBy(\"subtime_date\").sum().sort(\"subtime_date\").toPandas().to_excel('./收入.xlsx',index=False) # write.csv('./收入.csv')\n",
    "b.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "user_num_dict = defaultdict(list) # 生成一个元素为列表的字典\n",
    "product_a = ['TVMBHDYDBY25Y','TV4KBY20Y','TV4KBY20Y','TV4KBY20Y','TVMBHDY120LYB','TV4KBY20Y','TV4KBY20Y','TVMBHDYRJ199LYB']\n",
    "product_b = ['TVMBHRJDBY25Y','TVMBHRJBY20Y','TVMBHRJBY20Y','TVMBHRJBY20Y','TVMBHRJ120LYB','TVMBHRJBY20Y','TVMBHRJBY20Y']\n",
    "product_c = ['TVMBHDMDBY20Y','TV4KMBKDMBY10Y','TVMBHDMBY20Y','TV4KMBKDMBY10Y','TVMBHDMBY20Y','TV4KMBKDMBY10Y','TVMBHDMBY20Y',\n",
    "             'TVMBHDM99LYB','TVMBHDM120LYB','TV4KMBKDMBY10Y','TVMBHDMBY20Y','TV4KMBKDMBY10Y','TVMBHDMBY20Y']\n",
    "for year in ['2017', '2018', '2019']:\n",
    "    print(year)\n",
    "    for month in ['01','02','03','04','05','06','07','08','09','10','11','12']:\n",
    "        datetime = year + '-' + month\n",
    "        user_num_dict['月份'].append(datetime)\n",
    "        # spark.sql(\"SELECT count(distinct('userid')) FROM to_date_user\")\n",
    "        # user_num_dict['有效去重用户数'].append(results.filter(results.subtime_date <= datetime).filter(results.endtime_date > datetime).distinct().count())\n",
    "        total = 0\n",
    "        \n",
    "        revenue_a = a.filter(a.subtime_date == datetime).where(col(\"contractid\").isin(product_c)).agg({\"yuan\": \"sum\"}).toPandas().values[0][0]\n",
    "        revenue_b = b.filter(b.subtime_date <= datetime).filter(b.endtime_date >= datetime).where(col(\"contractid\").isin(product_c)).agg(\n",
    "            {\"yuan\": \"sum\"}).toPandas().values[0][0]\n",
    "        if revenue_a is not None:\n",
    "            total+=revenue_a\n",
    "        if revenue_b is not None:\n",
    "            total+=revenue_b\n",
    "        print(total)\n",
    "        user_num_dict['收入'].append(total)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "user_num_dict = defaultdict(list) # 生成一个元素为列表的字典\n",
    "for payway in ['903', '701', '56']:\n",
    "    for year in ['2015', '2016', '2017', '2018', '2019']:\n",
    "        user_num_dict['年份'].append(year)\n",
    "        year_total = 0\n",
    "        for month in ['01','02','03','04','05','06','07','08','09','10','11','12']:\n",
    "            datetime = year + '-' + month\n",
    "            month_total = 0\n",
    "            revenue_a = a.filter(a.subtime_date == datetime).filter(a.payway == payway).agg({\"yuan\": \"sum\"}).toPandas().values[0][0]\n",
    "            revenue_b = b.filter(b.subtime_date <= datetime).filter(b.endtime_date >= datetime).filter(a.payway == payway).agg({\"yuan\": \"sum\"}).toPandas().values[0][0]\n",
    "            if revenue_a is not None:\n",
    "                month_total += revenue_a\n",
    "            if revenue_b is not None:\n",
    "                month_total += revenue_b\n",
    "            year_total += month_total\n",
    "        print(year_total)\n",
    "        user_num_dict[payway].append(year_total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "user_num_dict = defaultdict(list) # 生成一个元素为列表的字典\n",
    "for year in ['2015', '2016', '2017', '2018', '2019']:\n",
    "    user_num_dict['年份'].append(year)\n",
    "    year_total = 0\n",
    "    for month in ['01','02','03','04','05','06','07','08','09','10','11','12']:\n",
    "        datetime = year + '-' + month\n",
    "        month_total = 0\n",
    "        revenue_b = b.filter(b.subtime_date <= datetime).filter(b.endtime_date >= datetime).agg({\"yuan\": \"sum\"}).toPandas().values[0][0]\n",
    "        if revenue_b is not None:\n",
    "            month_total += revenue_b\n",
    "        year_total += month_total\n",
    "    print(year_total)\n",
    "    user_num_dict[payway].append(year_total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(user_num_dict)\n",
    "# user_num_data=pd.DataFrame(user_num_dict) # 将字典转换为DataFrame，字典元素必须为列表类型\n",
    "# user_num_data.to_excel('./各年不同订购方式的收入.xlsx',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "user_num_dict = defaultdict(list) # 生成一个元素为列表的字典\n",
    "for year in ['2015', '2016', '2017', '2018', '2019']:\n",
    "    print(year)\n",
    "    for month in ['01','02','03','04','05','06','07','08','09','10','11','12']:\n",
    "        datetime = year + '-' + month\n",
    "        user_num_dict['月份'].append(datetime)\n",
    "        # spark.sql(\"SELECT count(distinct('userid')) FROM to_date_user\")\n",
    "        # user_num_dict['有效去重用户数'].append(results.filter(results.subtime_date <= datetime).filter(results.endtime_date > datetime).distinct().count())\n",
    "        total = 0\n",
    "        revenue_a = a.filter(a.subtime_date == datetime).agg({\"yuan\": \"sum\"}).toPandas().values[0][0]\n",
    "        revenue_b = b.filter(b.subtime_date <= datetime).filter(b.endtime_date >= datetime).agg({\"yuan\": \"sum\"}).toPandas().values[0][0]\n",
    "        if revenue_a is not None:\n",
    "            total+=revenue_a\n",
    "        if revenue_b is not None:\n",
    "            total+=revenue_b\n",
    "        print(revenue_a,revenue_b,total)\n",
    "        user_num_dict['收入'].append(total)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_num_data=pd.DataFrame(user_num_dict) # 将字典转换为DataFrame，字典元素必须为列表类型\n",
    "user_num_data.to_excel('./收入.xlsx',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.udf.register(\"strLen\", (str: String) => str.length())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results.select(\"subtime\")."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results.sort(results.userid.desc(), results.contractid.asc()).show(1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
