{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9050f10",
   "metadata": {},
   "outputs": [],
   "source": [
    "##  基础函数库\n",
    "import numpy as np \n",
    "import pandas as pd\n",
    "\n",
    "## 绘图函数库\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d14b16c",
   "metadata": {},
   "outputs": [],
   "source": [
    "## 我们利用Pandas自带的read_csv函数读取并转化为DataFrame格式\n",
    "data = pd.read_csv('./data_tx/train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9a8f9b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "## 利用.info()查看数据的整体信息\n",
    "data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56a0dc75",
   "metadata": {},
   "outputs": [],
   "source": [
    "## 进行简单的数据查看，我们可以利用 .head() 头部.tail()尾部\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1317401c",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.fillna(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "930c56b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f7c7a46",
   "metadata": {},
   "outputs": [],
   "source": [
    "numerical_features = [x for x in data.columns if data[x].dtype == np.float]\n",
    "category_features = [x for x in data.columns if data[x].dtype != np.float and x != 'RainTomorrow']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02a8afb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "category_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7903cebf",
   "metadata": {},
   "outputs": [],
   "source": [
    "## 选取三个特征与标签组合的散点可视化\n",
    "sns.pairplot(data=data[['Rainfall',\n",
    "'Evaporation',\n",
    "'Sunshine'] + ['RainTomorrow']], diag_kind='hist', hue= 'RainTomorrow')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acf69f49",
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in data[numerical_features].columns:\n",
    "    if col != 'RainTomorrow':\n",
    "        sns.boxplot(x='RainTomorrow', y=col, saturation=0.5, palette='pastel', data=data)\n",
    "        plt.title(col)\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23b9ac0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "tlog = {}\n",
    "for i in category_features:\n",
    "    tlog[i] = data[data['RainTomorrow'] == 'Yes'][i].value_counts()\n",
    "flog = {}\n",
    "for i in category_features:\n",
    "    flog[i] = data[data['RainTomorrow'] == 'No'][i].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84687c28",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,10))\n",
    "plt.subplot(1,2,1)\n",
    "plt.title('RainTomorrow')\n",
    "sns.barplot(x = pd.DataFrame(tlog['Location']).sort_index()['Location'], y = pd.DataFrame(tlog['Location']).sort_index().index, color = \"red\")\n",
    "plt.subplot(1,2,2)\n",
    "plt.title('Not RainTomorrow')\n",
    "sns.barplot(x = pd.DataFrame(flog['Location']).sort_index()['Location'], y = pd.DataFrame(flog['Location']).sort_index().index, color = \"blue\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b128365e",
   "metadata": {},
   "outputs": [],
   "source": [
    "## 把所有的相同类别的特征编码为同一个值\n",
    "def get_mapfunction(x):\n",
    "    mapp = dict(zip(x.unique().tolist(),\n",
    "         range(len(x.unique().tolist()))))\n",
    "    def mapfunction(y):\n",
    "        if y in mapp:\n",
    "            return mapp[y]\n",
    "        else:\n",
    "            return -1\n",
    "    return mapfunction\n",
    "for i in category_features:\n",
    "    data[i] = data[i].apply(get_mapfunction(data[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0d98caf",
   "metadata": {},
   "outputs": [],
   "source": [
    "## 编码后的字符串特征变成了数字\n",
    "data['Location'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10d4716b",
   "metadata": {},
   "outputs": [],
   "source": [
    "## 为了正确评估模型性能，将数据划分为训练集和测试集，并在训练集上训练模型，在测试集上验证模型性能。\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "## 选择其类别为0和1的样本 （不包括类别为2的样本）\n",
    "data_target_part = data['RainTomorrow']\n",
    "data_features_part = data[[x for x in data.columns if x != 'RainTomorrow']]\n",
    "\n",
    "## 测试集大小为20%， 80%/20%分\n",
    "x_train, x_test, y_train, y_test = train_test_split(data_features_part, data_target_part, test_size = 0.2, random_state = 2020)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b62a441",
   "metadata": {},
   "outputs": [],
   "source": [
    "## 导入XGBoost模型\n",
    "from xgboost.sklearn import XGBClassifier\n",
    "## 定义 XGBoost模型 \n",
    "clf = XGBClassifier()\n",
    "# 在训练集上训练XGBoost模型\n",
    "clf.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8c119f74",
   "metadata": {},
   "outputs": [],
   "source": [
    "from xgboost.sklearn import XGBClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dee785d4",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on class XGBClassifier in module xgboost.sklearn:\n",
      "\n",
      "class XGBClassifier(XGBModel, sklearn.base.ClassifierMixin)\n",
      " |  Implementation of the scikit-learn API for XGBoost classification.\n",
      " |  \n",
      " |  \n",
      " |  Parameters\n",
      " |  ----------\n",
      " |  \n",
      " |      n_estimators : int\n",
      " |          Number of boosting rounds.\n",
      " |      use_label_encoder : bool\n",
      " |          (Deprecated) Use the label encoder from scikit-learn to encode the labels. For new\n",
      " |          code, we recommend that you set this parameter to False.\n",
      " |  \n",
      " |      max_depth :  Optional[int]\n",
      " |          Maximum tree depth for base learners.\n",
      " |      learning_rate : Optional[float]\n",
      " |          Boosting learning rate (xgb's \"eta\")\n",
      " |      verbosity : Optional[int]\n",
      " |          The degree of verbosity. Valid values are 0 (silent) - 3 (debug).\n",
      " |      objective : typing.Union[str, typing.Callable[[numpy.ndarray, numpy.ndarray], typing.Tuple[numpy.ndarray, numpy.ndarray]], NoneType]\n",
      " |          Specify the learning task and the corresponding learning objective or\n",
      " |          a custom objective function to be used (see note below).\n",
      " |      booster: Optional[str]\n",
      " |          Specify which booster to use: gbtree, gblinear or dart.\n",
      " |      tree_method: Optional[str]\n",
      " |          Specify which tree method to use.  Default to auto.  If this parameter\n",
      " |          is set to default, XGBoost will choose the most conservative option\n",
      " |          available.  It's recommended to study this option from the parameters\n",
      " |          document: https://xgboost.readthedocs.io/en/latest/treemethod.html.\n",
      " |      n_jobs : Optional[int]\n",
      " |          Number of parallel threads used to run xgboost.  When used with other Scikit-Learn\n",
      " |          algorithms like grid search, you may choose which algorithm to parallelize and\n",
      " |          balance the threads.  Creating thread contention will significantly slow down both\n",
      " |          algorithms.\n",
      " |      gamma : Optional[float]\n",
      " |          Minimum loss reduction required to make a further partition on a leaf\n",
      " |          node of the tree.\n",
      " |      min_child_weight : Optional[float]\n",
      " |          Minimum sum of instance weight(hessian) needed in a child.\n",
      " |      max_delta_step : Optional[float]\n",
      " |          Maximum delta step we allow each tree's weight estimation to be.\n",
      " |      subsample : Optional[float]\n",
      " |          Subsample ratio of the training instance.\n",
      " |      colsample_bytree : Optional[float]\n",
      " |          Subsample ratio of columns when constructing each tree.\n",
      " |      colsample_bylevel : Optional[float]\n",
      " |          Subsample ratio of columns for each level.\n",
      " |      colsample_bynode : Optional[float]\n",
      " |          Subsample ratio of columns for each split.\n",
      " |      reg_alpha : Optional[float]\n",
      " |          L1 regularization term on weights (xgb's alpha).\n",
      " |      reg_lambda : Optional[float]\n",
      " |          L2 regularization term on weights (xgb's lambda).\n",
      " |      scale_pos_weight : Optional[float]\n",
      " |          Balancing of positive and negative weights.\n",
      " |      base_score : Optional[float]\n",
      " |          The initial prediction score of all instances, global bias.\n",
      " |      random_state : Optional[Union[numpy.random.RandomState, int]]\n",
      " |          Random number seed.\n",
      " |  \n",
      " |          .. note::\n",
      " |  \n",
      " |             Using gblinear booster with shotgun updater is nondeterministic as\n",
      " |             it uses Hogwild algorithm.\n",
      " |  \n",
      " |      missing : float, default np.nan\n",
      " |          Value in the data which needs to be present as a missing value.\n",
      " |      num_parallel_tree: Optional[int]\n",
      " |          Used for boosting random forest.\n",
      " |      monotone_constraints : Optional[Union[Dict[str, int], str]]\n",
      " |          Constraint of variable monotonicity.  See tutorial for more\n",
      " |          information.\n",
      " |      interaction_constraints : Optional[Union[str, List[Tuple[str]]]]\n",
      " |          Constraints for interaction representing permitted interactions.  The\n",
      " |          constraints must be specified in the form of a nest list, e.g. [[0, 1],\n",
      " |          [2, 3, 4]], where each inner list is a group of indices of features\n",
      " |          that are allowed to interact with each other.  See tutorial for more\n",
      " |          information\n",
      " |      importance_type: Optional[str]\n",
      " |          The feature importance type for the feature_importances\\_ property:\n",
      " |  \n",
      " |          * For tree model, it's either \"gain\", \"weight\", \"cover\", \"total_gain\" or\n",
      " |            \"total_cover\".\n",
      " |          * For linear model, only \"weight\" is defined and it's the normalized coefficients\n",
      " |            without bias.\n",
      " |  \n",
      " |      gpu_id : Optional[int]\n",
      " |          Device ordinal.\n",
      " |      validate_parameters : Optional[bool]\n",
      " |          Give warnings for unknown parameter.\n",
      " |      predictor : Optional[str]\n",
      " |          Force XGBoost to use specific predictor, available choices are [cpu_predictor,\n",
      " |          gpu_predictor].\n",
      " |      enable_categorical : bool\n",
      " |  \n",
      " |          .. versionadded:: 1.5.0\n",
      " |  \n",
      " |          Experimental support for categorical data.  Do not set to true unless you are\n",
      " |          interested in development. Only valid when `gpu_hist` and dataframe are used.\n",
      " |  \n",
      " |      kwargs : dict, optional\n",
      " |          Keyword arguments for XGBoost Booster object.  Full documentation of\n",
      " |          parameters can be found here:\n",
      " |          https://github.com/dmlc/xgboost/blob/master/doc/parameter.rst.\n",
      " |          Attempting to set a parameter via the constructor args and \\*\\*kwargs\n",
      " |          dict simultaneously will result in a TypeError.\n",
      " |  \n",
      " |          .. note:: \\*\\*kwargs unsupported by scikit-learn\n",
      " |  \n",
      " |              \\*\\*kwargs is unsupported by scikit-learn.  We do not guarantee\n",
      " |              that parameters passed via this argument will interact properly\n",
      " |              with scikit-learn.\n",
      " |  \n",
      " |          .. note::  Custom objective function\n",
      " |  \n",
      " |              A custom objective function can be provided for the ``objective``\n",
      " |              parameter. In this case, it should have the signature\n",
      " |              ``objective(y_true, y_pred) -> grad, hess``:\n",
      " |  \n",
      " |              y_true: array_like of shape [n_samples]\n",
      " |                  The target values\n",
      " |              y_pred: array_like of shape [n_samples]\n",
      " |                  The predicted values\n",
      " |  \n",
      " |              grad: array_like of shape [n_samples]\n",
      " |                  The value of the gradient for each sample point.\n",
      " |              hess: array_like of shape [n_samples]\n",
      " |                  The value of the second derivative for each sample point\n",
      " |  \n",
      " |  Method resolution order:\n",
      " |      XGBClassifier\n",
      " |      XGBModel\n",
      " |      sklearn.base.BaseEstimator\n",
      " |      sklearn.base.ClassifierMixin\n",
      " |      builtins.object\n",
      " |  \n",
      " |  Methods defined here:\n",
      " |  \n",
      " |  __init__(self, *, objective:Union[str, Callable[[numpy.ndarray, numpy.ndarray], Tuple[numpy.ndarray, numpy.ndarray]], NoneType]='binary:logistic', use_label_encoder:bool=True, **kwargs:Any) -> None\n",
      " |      Initialize self.  See help(type(self)) for accurate signature.\n",
      " |  \n",
      " |  evals_result(self) -> Dict[str, Dict[str, Union[List[float], List[Tuple[float, float]]]]]\n",
      " |      Return the evaluation results.\n",
      " |      \n",
      " |      If **eval_set** is passed to the `fit` function, you can call\n",
      " |      ``evals_result()`` to get evaluation results for all passed **eval_sets**.\n",
      " |      When **eval_metric** is also passed to the `fit` function, the\n",
      " |      **evals_result** will contain the **eval_metrics** passed to the `fit` function.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      evals_result : dictionary\n",
      " |      \n",
      " |      Example\n",
      " |      -------\n",
      " |      \n",
      " |      .. code-block:: python\n",
      " |      \n",
      " |          param_dist = {'objective':'binary:logistic', 'n_estimators':2}\n",
      " |      \n",
      " |          clf = xgb.XGBClassifier(**param_dist)\n",
      " |      \n",
      " |          clf.fit(X_train, y_train,\n",
      " |                  eval_set=[(X_train, y_train), (X_test, y_test)],\n",
      " |                  eval_metric='logloss',\n",
      " |                  verbose=True)\n",
      " |      \n",
      " |          evals_result = clf.evals_result()\n",
      " |      \n",
      " |      The variable **evals_result** will contain\n",
      " |      \n",
      " |      .. code-block:: python\n",
      " |      \n",
      " |          {'validation_0': {'logloss': ['0.604835', '0.531479']},\n",
      " |          'validation_1': {'logloss': ['0.41965', '0.17686']}}\n",
      " |  \n",
      " |  fit(self, X:Any, y:Any, *, sample_weight:Union[Any, NoneType]=None, base_margin:Union[Any, NoneType]=None, eval_set:Union[List[Tuple[Any, Any]], NoneType]=None, eval_metric:Union[str, List[str], Callable[[numpy.ndarray, xgboost.core.DMatrix], Tuple[str, float]], NoneType]=None, early_stopping_rounds:Union[int, NoneType]=None, verbose:Union[bool, NoneType]=True, xgb_model:Union[xgboost.core.Booster, str, xgboost.sklearn.XGBModel, NoneType]=None, sample_weight_eval_set:Union[List[Any], NoneType]=None, base_margin_eval_set:Union[List[Any], NoneType]=None, feature_weights:Union[Any, NoneType]=None, callbacks:Union[List[xgboost.callback.TrainingCallback], NoneType]=None) -> 'XGBClassifier'\n",
      " |      Fit gradient boosting classifier.\n",
      " |      \n",
      " |      Note that calling ``fit()`` multiple times will cause the model object to be\n",
      " |      re-fit from scratch. To resume training from a previous checkpoint, explicitly\n",
      " |      pass ``xgb_model`` argument.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X :\n",
      " |          Feature matrix\n",
      " |      y :\n",
      " |          Labels\n",
      " |      sample_weight :\n",
      " |          instance weights\n",
      " |      base_margin :\n",
      " |          global bias for each instance.\n",
      " |      eval_set :\n",
      " |          A list of (X, y) tuple pairs to use as validation sets, for which\n",
      " |          metrics will be computed.\n",
      " |          Validation metrics will help us track the performance of the model.\n",
      " |      eval_metric :\n",
      " |          If a str, should be a built-in evaluation metric to use. See doc/parameter.rst.\n",
      " |      \n",
      " |          If a list of str, should be the list of multiple built-in evaluation metrics\n",
      " |          to use.\n",
      " |      \n",
      " |          If callable, a custom evaluation metric. The call signature is\n",
      " |          ``func(y_predicted, y_true)`` where ``y_true`` will be a DMatrix object such\n",
      " |          that you may need to call the ``get_label`` method. It must return a str,\n",
      " |          value pair where the str is a name for the evaluation and value is the value\n",
      " |          of the evaluation function. The callable custom objective is always minimized.\n",
      " |      early_stopping_rounds :\n",
      " |          Activates early stopping. Validation metric needs to improve at least once in\n",
      " |          every **early_stopping_rounds** round(s) to continue training.\n",
      " |          Requires at least one item in **eval_set**.\n",
      " |      \n",
      " |          The method returns the model from the last iteration (not the best one).\n",
      " |          If there's more than one item in **eval_set**, the last entry will be used\n",
      " |          for early stopping.\n",
      " |      \n",
      " |          If there's more than one metric in **eval_metric**, the last metric will be\n",
      " |          used for early stopping.\n",
      " |      \n",
      " |          If early stopping occurs, the model will have three additional fields:\n",
      " |          ``clf.best_score``, ``clf.best_iteration``.\n",
      " |      verbose :\n",
      " |          If `verbose` and an evaluation set is used, writes the evaluation metric\n",
      " |          measured on the validation set to stderr.\n",
      " |      xgb_model :\n",
      " |          file name of stored XGBoost model or 'Booster' instance XGBoost model to be\n",
      " |          loaded before training (allows training continuation).\n",
      " |      sample_weight_eval_set :\n",
      " |          A list of the form [L_1, L_2, ..., L_n], where each L_i is an array like\n",
      " |          object storing instance weights for the i-th validation set.\n",
      " |      base_margin_eval_set :\n",
      " |          A list of the form [M_1, M_2, ..., M_n], where each M_i is an array like\n",
      " |          object storing base margin for the i-th validation set.\n",
      " |      feature_weights :\n",
      " |          Weight for each feature, defines the probability of each feature being\n",
      " |          selected when colsample is being used.  All values must be greater than 0,\n",
      " |          otherwise a `ValueError` is thrown.  Only available for `hist`, `gpu_hist` and\n",
      " |          `exact` tree methods.\n",
      " |      callbacks :\n",
      " |          List of callback functions that are applied at end of each iteration.\n",
      " |          It is possible to use predefined callbacks by using :ref:`callback_api`.\n",
      " |          Example:\n",
      " |      \n",
      " |          .. code-block:: python\n",
      " |      \n",
      " |              callbacks = [xgb.callback.EarlyStopping(rounds=early_stopping_rounds,\n",
      " |                                                      save_best=True)]\n",
      " |  \n",
      " |  predict(self, X:Any, output_margin:bool=False, ntree_limit:Union[int, NoneType]=None, validate_features:bool=True, base_margin:Union[Any, NoneType]=None, iteration_range:Union[Tuple[int, int], NoneType]=None) -> numpy.ndarray\n",
      " |      Predict with `X`.  If the model is trained with early stopping, then `best_iteration`\n",
      " |      is used automatically.  For tree models, when data is on GPU, like cupy array or\n",
      " |      cuDF dataframe and `predictor` is not specified, the prediction is run on GPU\n",
      " |      automatically, otherwise it will run on CPU.\n",
      " |      \n",
      " |      .. note:: This function is only thread safe for `gbtree` and `dart`.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X :\n",
      " |          Data to predict with.\n",
      " |      output_margin :\n",
      " |          Whether to output the raw untransformed margin value.\n",
      " |      ntree_limit :\n",
      " |          Deprecated, use `iteration_range` instead.\n",
      " |      validate_features :\n",
      " |          When this is True, validate that the Booster's and data's feature_names are\n",
      " |          identical.  Otherwise, it is assumed that the feature_names are the same.\n",
      " |      base_margin :\n",
      " |          Margin added to prediction.\n",
      " |      iteration_range :\n",
      " |          Specifies which layer of trees are used in prediction.  For example, if a\n",
      " |          random forest is trained with 100 rounds.  Specifying ``iteration_range=(10,\n",
      " |          20)``, then only the forests built during [10, 20) (half open set) rounds are\n",
      " |          used in this prediction.\n",
      " |      \n",
      " |          .. versionadded:: 1.4.0\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      prediction\n",
      " |  \n",
      " |  predict_proba(self, X:Any, ntree_limit:Union[int, NoneType]=None, validate_features:bool=True, base_margin:Union[Any, NoneType]=None, iteration_range:Union[Tuple[int, int], NoneType]=None) -> numpy.ndarray\n",
      " |      Predict the probability of each `X` example being of a given class.\n",
      " |      \n",
      " |      .. note:: This function is only thread safe for `gbtree` and `dart`.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : array_like\n",
      " |          Feature matrix.\n",
      " |      ntree_limit : int\n",
      " |          Deprecated, use `iteration_range` instead.\n",
      " |      validate_features : bool\n",
      " |          When this is True, validate that the Booster's and data's feature_names are\n",
      " |          identical.  Otherwise, it is assumed that the feature_names are the same.\n",
      " |      base_margin : array_like\n",
      " |          Margin added to prediction.\n",
      " |      iteration_range :\n",
      " |          Specifies which layer of trees are used in prediction.  For example, if a\n",
      " |          random forest is trained with 100 rounds.  Specifying `iteration_range=(10,\n",
      " |          20)`, then only the forests built during [10, 20) (half open set) rounds are\n",
      " |          used in this prediction.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      prediction :\n",
      " |          a numpy array of shape array-like of shape (n_samples, n_classes) with the\n",
      " |          probability of each data example being of a given class.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from XGBModel:\n",
      " |  \n",
      " |  __sklearn_is_fitted__(self) -> bool\n",
      " |  \n",
      " |  apply(self, X:Any, ntree_limit:int=0, iteration_range:Union[Tuple[int, int], NoneType]=None) -> numpy.ndarray\n",
      " |      Return the predicted leaf every tree for each sample. If the model is trained with\n",
      " |      early stopping, then `best_iteration` is used automatically.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : array_like, shape=[n_samples, n_features]\n",
      " |          Input features matrix.\n",
      " |      \n",
      " |      iteration_range :\n",
      " |          See :py:meth:`xgboost.XGBRegressor.predict`.\n",
      " |      \n",
      " |      ntree_limit :\n",
      " |          Deprecated, use ``iteration_range`` instead.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      X_leaves : array_like, shape=[n_samples, n_trees]\n",
      " |          For each datapoint x in X and for each tree, return the index of the\n",
      " |          leaf x ends up in. Leaves are numbered within\n",
      " |          ``[0; 2**(self.max_depth+1))``, possibly with gaps in the numbering.\n",
      " |  \n",
      " |  get_booster(self) -> xgboost.core.Booster\n",
      " |      Get the underlying xgboost Booster of this model.\n",
      " |      \n",
      " |      This will raise an exception when fit was not called\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      booster : a xgboost booster of underlying model\n",
      " |  \n",
      " |  get_num_boosting_rounds(self) -> int\n",
      " |      Gets the number of xgboost boosting rounds.\n",
      " |  \n",
      " |  get_params(self, deep:bool=True) -> Dict[str, Any]\n",
      " |      Get parameters.\n",
      " |  \n",
      " |  get_xgb_params(self) -> Dict[str, Any]\n",
      " |      Get xgboost specific parameters.\n",
      " |  \n",
      " |  load_model(self, fname:Union[str, bytearray, os.PathLike]) -> None\n",
      " |      Load the model from a file or bytearray. Path to file can be local\n",
      " |      or as an URI.\n",
      " |      \n",
      " |      The model is loaded from XGBoost format which is universal among the various\n",
      " |      XGBoost interfaces. Auxiliary attributes of the Python Booster object (such as\n",
      " |      feature_names) will not be loaded when using binary format.  To save those\n",
      " |      attributes, use JSON instead.  See: `Model IO\n",
      " |      <https://xgboost.readthedocs.io/en/stable/tutorials/saving_model.html>`_ for more\n",
      " |      info.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      fname :\n",
      " |          Input file name or memory buffer(see also save_raw)\n",
      " |  \n",
      " |  save_model(self, fname:Union[str, os.PathLike]) -> None\n",
      " |      Save the model to a file.\n",
      " |      \n",
      " |      The model is saved in an XGBoost internal format which is universal among the\n",
      " |      various XGBoost interfaces. Auxiliary attributes of the Python Booster object\n",
      " |      (such as feature_names) will not be saved when using binary format.  To save those\n",
      " |      attributes, use JSON instead. See: `Model IO\n",
      " |      <https://xgboost.readthedocs.io/en/stable/tutorials/saving_model.html>`_ for more\n",
      " |      info.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      fname : string or os.PathLike\n",
      " |          Output file name\n",
      " |  \n",
      " |  set_params(self, **params:Any) -> 'XGBModel'\n",
      " |      Set the parameters of this estimator.  Modification of the sklearn method to\n",
      " |      allow unknown kwargs. This allows using the full range of xgboost\n",
      " |      parameters that are not defined as member variables in sklearn grid\n",
      " |      search.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      self\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors inherited from XGBModel:\n",
      " |  \n",
      " |  best_iteration\n",
      " |  \n",
      " |  best_ntree_limit\n",
      " |  \n",
      " |  best_score\n",
      " |  \n",
      " |  coef_\n",
      " |      Coefficients property\n",
      " |      \n",
      " |      .. note:: Coefficients are defined only for linear learners\n",
      " |      \n",
      " |          Coefficients are only defined when the linear model is chosen as\n",
      " |          base learner (`booster=gblinear`). It is not defined for other base\n",
      " |          learner types, such as tree learners (`booster=gbtree`).\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      coef_ : array of shape ``[n_features]`` or ``[n_classes, n_features]``\n",
      " |  \n",
      " |  feature_importances_\n",
      " |      Feature importances property, return depends on `importance_type` parameter.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      feature_importances_ : array of shape ``[n_features]`` except for multi-class\n",
      " |      linear model, which returns an array with shape `(n_features, n_classes)`\n",
      " |  \n",
      " |  intercept_\n",
      " |      Intercept (bias) property\n",
      " |      \n",
      " |      .. note:: Intercept is defined only for linear learners\n",
      " |      \n",
      " |          Intercept (bias) is only defined when the linear model is chosen as base\n",
      " |          learner (`booster=gblinear`). It is not defined for other base learner types, such\n",
      " |          as tree learners (`booster=gbtree`).\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      intercept_ : array of shape ``(1,)`` or ``[n_classes]``\n",
      " |  \n",
      " |  n_features_in_\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from sklearn.base.BaseEstimator:\n",
      " |  \n",
      " |  __getstate__(self)\n",
      " |  \n",
      " |  __repr__(self, N_CHAR_MAX=700)\n",
      " |      Return repr(self).\n",
      " |  \n",
      " |  __setstate__(self, state)\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors inherited from sklearn.base.BaseEstimator:\n",
      " |  \n",
      " |  __dict__\n",
      " |      dictionary for instance variables (if defined)\n",
      " |  \n",
      " |  __weakref__\n",
      " |      list of weak references to the object (if defined)\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from sklearn.base.ClassifierMixin:\n",
      " |  \n",
      " |  score(self, X, y, sample_weight=None)\n",
      " |      Return the mean accuracy on the given test data and labels.\n",
      " |      \n",
      " |      In multi-label classification, this is the subset accuracy\n",
      " |      which is a harsh metric since you require for each sample that\n",
      " |      each label set be correctly predicted.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : array-like of shape (n_samples, n_features)\n",
      " |          Test samples.\n",
      " |      \n",
      " |      y : array-like of shape (n_samples,) or (n_samples, n_outputs)\n",
      " |          True labels for `X`.\n",
      " |      \n",
      " |      sample_weight : array-like of shape (n_samples,), default=None\n",
      " |          Sample weights.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      score : float\n",
      " |          Mean accuracy of ``self.predict(X)`` wrt. `y`.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(XGBClassifier)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33d0f7fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "## 在训练集和测试集上分布利用训练好的模型进行预测\n",
    "train_predict = clf.predict(x_train)\n",
    "test_predict = clf.predict(x_test)\n",
    "from sklearn import metrics\n",
    "\n",
    "## 利用accuracy（准确度）【预测正确的样本数目占总预测样本数目的比例】评估模型效果\n",
    "print('The accuracy of the Logistic Regression is:',metrics.accuracy_score(y_train,train_predict))\n",
    "print('The accuracy of the Logistic Regression is:',metrics.accuracy_score(y_test,test_predict))\n",
    "\n",
    "## 查看混淆矩阵 (预测值和真实值的各类情况统计矩阵)\n",
    "confusion_matrix_result = metrics.confusion_matrix(test_predict,y_test)\n",
    "print('The confusion matrix result:\\n',confusion_matrix_result)\n",
    "\n",
    "# 利用热力图对于结果进行可视化\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(confusion_matrix_result, annot=True, cmap='Blues')\n",
    "plt.xlabel('Predicted labels')\n",
    "plt.ylabel('True labels')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "970d600f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sns.barplot(y=data_features_part.columns, x=clf.feature_importances_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bc734ec",
   "metadata": {},
   "source": [
    "此外，我们还可以使用XGBoost中的下列重要属性来评估特征的重要性。\n",
    "weight:是以特征用到的次数来评价\n",
    "gain:当利用特征做划分的时候的评价基尼指数\n",
    "cover:利用一个覆盖样本的指标二阶导数（具体原理不清楚有待探究）平均值来划分。\n",
    "total_gain:总基尼指数\n",
    "total_cover:总覆盖"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a6ad4f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "from xgboost import plot_importance\n",
    "\n",
    "def estimate(model,data):\n",
    "\n",
    "    #sns.barplot(data.columns,model.feature_importances_)\n",
    "    ax1=plot_importance(model,importance_type=\"gain\")\n",
    "    ax1.set_title('gain')\n",
    "    ax2=plot_importance(model, importance_type=\"weight\")\n",
    "    ax2.set_title('weight')\n",
    "    ax3 = plot_importance(model, importance_type=\"cover\")\n",
    "    ax3.set_title('cover')\n",
    "    plt.show()\n",
    "def classes(data,label,test):\n",
    "    model=XGBClassifier()\n",
    "    model.fit(data,label)\n",
    "    ans=model.predict(test)\n",
    "    estimate(model, data)\n",
    "    return ans\n",
    " \n",
    "ans=classes(x_train,y_train,x_test)\n",
    "pre=accuracy_score(y_test, ans)\n",
    "print('acc=',accuracy_score(y_test,ans))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eaf761ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "## 从sklearn库中导入网格调参函数\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "## 定义参数取值范围\n",
    "learning_rate = [0.1, 0.3, 0.6]\n",
    "subsample = [0.8, 0.9]\n",
    "colsample_bytree = [0.6, 0.8]\n",
    "max_depth = [3,5,8]\n",
    "\n",
    "parameters = { 'learning_rate': learning_rate,\n",
    "              'subsample': subsample,\n",
    "              'colsample_bytree':colsample_bytree,\n",
    "              'max_depth': max_depth}\n",
    "model = XGBClassifier(n_estimators = 50)\n",
    "\n",
    "## 进行网格搜索\n",
    "clf = GridSearchCV(model, parameters, cv=3, scoring='accuracy',verbose=1,n_jobs=-1)\n",
    "clf = clf.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6452c51a",
   "metadata": {},
   "outputs": [],
   "source": [
    "## 网格搜索后的最好参数为\n",
    "clf.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17b37954",
   "metadata": {},
   "outputs": [],
   "source": [
    "## 在训练集和测试集上分布利用最好的模型参数进行预测\n",
    "\n",
    "## 定义带参数的 XGBoost模型 \n",
    "clf = XGBClassifier(colsample_bytree = 0.6, learning_rate = 0.3, max_depth= 8, subsample = 0.9)\n",
    "# 在训练集上训练XGBoost模型\n",
    "clf.fit(x_train, y_train)\n",
    "\n",
    "train_predict = clf.predict(x_train)\n",
    "test_predict = clf.predict(x_test)\n",
    "\n",
    "## 利用accuracy（准确度）【预测正确的样本数目占总预测样本数目的比例】评估模型效果\n",
    "print('The accuracy of the Logistic Regression is:',metrics.accuracy_score(y_train,train_predict))\n",
    "print('The accuracy of the Logistic Regression is:',metrics.accuracy_score(y_test,test_predict))\n",
    "\n",
    "## 查看混淆矩阵 (预测值和真实值的各类情况统计矩阵)\n",
    "confusion_matrix_result = metrics.confusion_matrix(test_predict,y_test)\n",
    "print('The confusion matrix result:\\n',confusion_matrix_result)\n",
    "\n",
    "# 利用热力图对于结果进行可视化\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(confusion_matrix_result, annot=True, cmap='Blues')\n",
    "plt.xlabel('Predicted labels')\n",
    "plt.ylabel('True labels')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bc90c4b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
